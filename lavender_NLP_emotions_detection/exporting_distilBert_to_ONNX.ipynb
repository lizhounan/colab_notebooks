{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"exporting_distilBert_to_ONNX.ipynb","provenance":[],"mount_file_id":"11ennGI3QbEy-Zc7JHnKbaZG7d-CoWht6","authorship_tag":"ABX9TyOiIlWQyUf2CTvWyp2s2RaZ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"lhMV1r5pVwnK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604612493024,"user_tz":300,"elapsed":2885,"user":{"displayName":"Zhounan Li","photoUrl":"","userId":"12163187697439231323"}},"outputId":"fdf11058-d85a-47ca-b020-7a01c5158e75"},"source":["! pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html\n","Requirement already satisfied: torch_nightly in /usr/local/lib/python3.6/dist-packages (1.2.0.dev20190805)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CcmsiySK4yc8","executionInfo":{"status":"ok","timestamp":1606184050414,"user_tz":300,"elapsed":2412,"user":{"displayName":"Zhounan Li","photoUrl":"","userId":"12163187697439231323"}}},"source":["import torch.onnx"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"_f3o40LYAsVS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606184059971,"user_tz":300,"elapsed":8500,"user":{"displayName":"Zhounan Li","photoUrl":"","userId":"12163187697439231323"}},"outputId":"43774fde-961d-441c-a0e2-9ae426caba9a"},"source":["! pip install transformers==3.0.2"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers==3.0.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\u001b[K     |████████████████████████████████| 778kB 5.8MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 4.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.18.5)\n","Collecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 27.3MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 44.0MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (0.17.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.11.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=a31488f54ed23ce8741facb12e65303ff40732e5892e26825052747b44652e79\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.8.1rc1 transformers-3.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1AtmWsrR54lL","executionInfo":{"status":"ok","timestamp":1606184063122,"user_tz":300,"elapsed":3149,"user":{"displayName":"Zhounan Li","photoUrl":"","userId":"12163187697439231323"}}},"source":["import warnings\n","warnings.simplefilter('ignore')\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from sklearn import metrics\n","import transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import DistilBertTokenizer, DistilBertModel\n","import logging\n","logging.basicConfig(level=logging.ERROR)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"3MmfcTJJAVim","executionInfo":{"status":"ok","timestamp":1606184063124,"user_tz":300,"elapsed":3147,"user":{"displayName":"Zhounan Li","photoUrl":"","userId":"12163187697439231323"}}},"source":["class DistilBERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(DistilBERTClass, self).__init__()\n","        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n","        self.pre_classifier = torch.nn.Linear(768, 768)\n","        self.dropout = torch.nn.Dropout(0.5)\n","        self.classifier = torch.nn.Linear(768, 4)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_state = output_1[0]\n","        pooler = hidden_state[:, 0]\n","        pooler = self.pre_classifier(pooler)\n","        pooler = torch.nn.Tanh()(pooler)\n","        pooler = self.dropout(pooler)\n","        output = self.classifier(pooler)\n","        return output"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"QStXds3DAZTP","executionInfo":{"status":"ok","timestamp":1606184989389,"user_tz":300,"elapsed":1865,"user":{"displayName":"Zhounan Li","photoUrl":"","userId":"12163187697439231323"}}},"source":["model  = DistilBERTClass()"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGCKASgiAckx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606184996022,"user_tz":300,"elapsed":900,"user":{"displayName":"Zhounan Li","photoUrl":"","userId":"12163187697439231323"}},"outputId":"f99cef53-dd9c-4c0d-fb90-202b788d832a"},"source":["model.load_state_dict(torch.load('/content/drive/MyDrive/distilbert_demo_emotions_state_dict_11_17_epoch2', map_location=torch.device('cpu')), strict=False)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"x45K-60SN1bW","executionInfo":{"status":"ok","timestamp":1606185003432,"user_tz":300,"elapsed":292,"user":{"displayName":"Zhounan Li","photoUrl":"","userId":"12163187697439231323"}}},"source":["model = torch.nn.DataParallel(model)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQCTCakyA6jS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606185004438,"user_tz":300,"elapsed":322,"user":{"displayName":"Zhounan Li","photoUrl":"","userId":"12163187697439231323"}},"outputId":"7b5bb538-fd78-4db9-c9c6-0e46b1490a49"},"source":["model.eval()"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataParallel(\n","  (module): DistilBERTClass(\n","    (l1): DistilBertModel(\n","      (embeddings): Embeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (transformer): Transformer(\n","        (layer): ModuleList(\n","          (0): TransformerBlock(\n","            (attention): MultiHeadSelfAttention(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (ffn): FFN(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","          (1): TransformerBlock(\n","            (attention): MultiHeadSelfAttention(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (ffn): FFN(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","          (2): TransformerBlock(\n","            (attention): MultiHeadSelfAttention(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (ffn): FFN(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","          (3): TransformerBlock(\n","            (attention): MultiHeadSelfAttention(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (ffn): FFN(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","          (4): TransformerBlock(\n","            (attention): MultiHeadSelfAttention(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (ffn): FFN(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","          (5): TransformerBlock(\n","            (attention): MultiHeadSelfAttention(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (ffn): FFN(\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","        )\n","      )\n","    )\n","    (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","    (classifier): Linear(in_features=768, out_features=4, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"xMVkluWXCCnG","executionInfo":{"status":"ok","timestamp":1606185006238,"user_tz":300,"elapsed":294,"user":{"displayName":"Zhounan Li","photoUrl":"","userId":"12163187697439231323"}}},"source":["tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"cmFfhuTeBKJY","executionInfo":{"status":"ok","timestamp":1606185007262,"user_tz":300,"elapsed":511,"user":{"displayName":"Zhounan Li","photoUrl":"","userId":"12163187697439231323"}}},"source":["text = 'this is an challenging task'\n","input = tokenizer.encode_plus(\n","              text,\n","              None,\n","              add_special_tokens=True,\n","              max_length=100,\n","              pad_to_max_length=True,\n","              return_token_type_ids=True\n","          )\n","ids = torch.tensor([input['input_ids']], dtype=torch.long)\n","mask = torch.tensor([input['attention_mask']], dtype=torch.long)\n","token_type_ids = torch.tensor([input[\"token_type_ids\"]], dtype=torch.long)\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"soqPgTiJB_EP","executionInfo":{"status":"ok","timestamp":1606185107292,"user_tz":300,"elapsed":3553,"user":{"displayName":"Zhounan Li","photoUrl":"","userId":"12163187697439231323"}}},"source":["torch.onnx.export(\n","    model.module,\n","    (ids, mask, token_type_ids),\n","    '/content/drive/MyDrive/distilbert_emotions_detection.onnx',\n","    input_names=['ids', 'mask', 'ttids'],\n","    output_names= ['output'],\n","    dynamic_axes={\n","        'ids': {0: 'batch_size'}, \n","        'mask': {0: 'batch_size'}, \n","        'ttids': {0: 'batch_size'}, \n","        'output': {0: 'batch_size'}\n","    }\n",")"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPLM-DAhSY6D"},"source":[""],"execution_count":null,"outputs":[]}]}